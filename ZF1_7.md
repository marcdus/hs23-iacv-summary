# ZF

- Lecture 1: Camera Model, Light Interaction with matter, Illumination techniques
    - Interaction with Matter
        - Absorption
            
            Light absorption is a process by which the energy of photons is taken up by electrons in the atoms or molecules of a material. An object appears a certain color because it absorbs certain wavelengths of light and reflects or transmits others. 
            
        - Scattering
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled.png)
            
            **Scattering** occurs when particles or large gas molecules present in the atmosphere interact with and cause the electromagnetic radiation to be redirected from its original path. How much scattering takes place depends on several factors including the wavelength of the radiation, and the distance the radiation travels through the atmosphere. There are three types of scattering which take place.
            
            - Rayleigh scattering
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%201.png)
                
                **Rayleigh scattering** occurs when particles are very small compared to the wavelength of the radiation. These could be particles such as small specks of dust or nitrogen and oxygen molecules. Rayleigh scattering causes shorter wavelengths of energy to be scattered much more than longer wavelengths. Rayleigh scattering is the dominant scattering mechanism in the upper atmosphere. The fact that the sky appears "blue" during the day is because of this phenomenon. As sunlight passes through the atmosphere, the shorter wavelengths (i.e. blue) of the visible spectrum are scattered more than the other (longer) visible wavelengths. At **sunrise and sunset** the light has to travel farther through the atmosphere than at midday and the scattering of the shorter wavelengths is more complete; this leaves a greater proportion of the longer wavelengths to penetrate the atmosphere.
                
            - Mie scattering
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%202.png)
                
                **Mie scattering** occurs when the particles are just about the same size as the wavelength of the radiation. Dust, pollen, smoke and water vapour are common causes of Mie scattering which tends to affect longer wavelengths than those affected by Rayleigh scattering. Mie scattering occurs mostly in the lower portions of the atmosphere where larger particles are more abundant, and dominates when cloud conditions are overcast.
                
            - Nonselective scattering
                
                The final scattering mechanism of importance is called **nonselective scattering**. This occurs when the particles are much larger than the wavelength of the radiation. Water droplets and large dust particles can cause this type of scattering. Nonselective scattering gets its name from the fact that all wavelengths are scattered about equally. This type of scattering causes fog and clouds to appear white to our eyes because blue, green, and red light are all scattered in approximately equal quantities (blue+green+red light = white light).
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%203.png)
                
        - Reflection
            
            The type of the reflection depends on the type of the surface. We differentiate between diffuse (lambertian), and specular refelection. Normally, a mixed reflexion occures. 
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%204.png)
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%205.png)
            
        - Refraction
            
            Refraction is a phenomenon that occurs when light waves travel through a medium with a different optical density, which means that light is travelling with a different speed of light. The relationship between the angles of incidence (the angle at which light enters a medium) and refraction is described by Snell's Law:
            
            $$
            n_1sin(\theta_1) = n_2sin(\theta_2)
            $$
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%206.png)
            
            > **Good to know:**
            The refractive index n is wavelength-dependent (dispersion). Shorter wavelength gets bended most (higher n).
            > 
            > 
            > ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%207.png)
            > 
        
        ---
        
    - Illumination techniques
        - Backlighting
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%208.png)
            
            - Lamps placed behind a transmitting diffuser plate
            - Light source behind the object
            - Generates high-contrast
            silhouette images, easy to
            handle with binary images
            - Often used in inspection
        - Directional lighting
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%209.png)
            
            - Generate sharp shadows
            - Generate specular reflection (e.g., crack detection)
            - Shadows and shading yield information about shape
        - Diffuse lighting
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2010.png)
            
            - Uniform illumination from all directions
            - Prevents sharp shadows and large intensity variations over glossy surfaces.
            - All directions contribute to extra diffuse reflection, but contributions to the specular peak arise from directions close to the mirror one only
        - Polarized lighting
            
            **Polarized light:**
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2011.png)
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2012.png)
            
            **Usecases:**
            
            - To improve contrast between diffuse and specular reflections
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2013.png)
                
                - Specular reflection keeps polarization
                - Diffuse reflection depolarizes
                - We can suppress specular reflection by having a crossed polarizer / analyzer pair.
                - Prevents the large dynamic range caused by glare from the specular reflection
            - To improve contrasts between dielectrics and metals
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2014.png)
                
                - Distinguishing metals from dielectrics
                - Distinction between specular reflection from dielectrics and metals
                - At the Brewster angle for the dielectric: reflect from dielectric has no parallel component → Polarized light.
                - Metal has both parallel and perpendicular.
                - Dielectrics should appear dark, metals bright.
            
        
        ---
        
    - Cameras
        - Pinhole Model
            
            It is based on the concept of a pinhole camera, which is essentially a light-tight box with a small aperture (the "pinhole") on one side, allowing light to enter and form an inverted image on the opposite side. The model assumes that light travels in straight lines and that there is no lens distortion.
            
            $$
            \frac{X_i}{X_0}= \frac{Y_i}{Y_0}=\frac{f}{-Z_0}=-m
            $$
            
            , with focal length $f$ and linear magnification $m$
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2015.png)
            
        - Thin lens model
            
            The thin lens model assumes that the lens is very thin compared to its focal length and that it has a spherical surface. It yields brighter images through lens that captures more light. This is still like having a pinhole camera as the image is still inverted and the pinhole at the lens center. This model has better light sensitivity, but  the depth of focus (DoF) is reduced . It is described by the lens equation: 
            
            $$
            \frac{1}{|Z_0|}+\frac{1}{|Z_i|} = \frac{1}{f}
            $$
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2016.png)
            
            **Depth of Focus:**
            
            The depth of focus refers to the range of distances along the optical axis within which objects will be in acceptable focus.
            
            $$
            DoF = \Delta P_0^- = P_0 - P_0^- = \frac{P_0(P_0-f)}{P_0+fd/b-f}
            $$
            
            with lens diameter $d$ (how much light comes in), sensor size $b$. 
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2017.png)
            
        - Perspective projection
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2018.png)
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2019.png)
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2020.png)
            
        
        ---
        
- Lecture 2: Digitalisation in a Camera, Image Enhancements
    - Discretization/Digitalization
        - 1) Sampling – spatial discretization, creates “pixels”
            
            Sampling schemes: 
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2021.png)
            
            - Step 1: Integrate brightness over a cell window
                
                → Leads to blurring type degradation
                
                For integrate the brightness over a cell, we convolute
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2022.png)
                
                **Convolution**
                
                In discrete domain, a convolution is just a kernel. In continuous domain, the convolution is defined by: 
                
                $$
                o(x', y')= \iint i(x,y)p(x-x', y-y')dxdy
                $$
                
                with $i$ the image intensity and $p$ the averaging function (point spread function).
                
                In the discrete domain, the convolution (what we do with a kernel) is defined as: 
                
                $$
                o(i', j')= \sum_{i=1}^M \sum_{j=1}^N I(i,j)p(i-i', j-j')
                $$
                
                with $I$ the image and $p$ the kernel/mask/filter, $M,N$ the size of the image and $i, j$ the row and column of the image.
                
                **Fourier Transformation**
                
                The Fourier Transformation is a linear transformation from the spatial domain into the frequency domain. In the frequency domain, the convolution of two functions in the spatial domain is just its multiplication. 
                
                $$
                F(u, v) = \int_{-\infty}^\infty \int_{-\infty}^\infty f(x, y) \cdot e^{-i(2\pi(ux + vy))} \,dx\,dy
                $$
                
                $$
                F^{-1}(u, v) = f(x, y) = \int_{-\infty}^\infty \int_{-\infty}^\infty  F(u, v) \cdot e^{i(2\pi(ux + vy))} \,du\,dv
                $$
                
                uv-plane → frequency domain 
                
                xy-plane → spatial domain
                
                ---
                
                The image in frequency domain consists of a magnitude and a phase, which both are important. 
                
                Magnitude:
                
                $$
                |F(u, v)| = \sqrt{\text{Re}(F(u, v))^2 + \text{Im}(F(u, v))^2}
                $$
                
                Phase-angle:
                
                $$
                \phi(u, v) = \arctan\left(\frac{\text{Im}(F(u, v))}{\text{Re}(F(u, v))}\right)
                $$
                
            - Step 2: Read out values only at the pixel centers
                
                → Leads to aliasing and leakage, frequency domain issues
                
                - When we read off one value per pixel area, we are losing information on the image indefinitely, if the image is not band-limited, which is almost always the case.
                - The information we lose is on the higher frequencies, meaning very fine details on edges, corners and texture patterns.
                - The lowest possible sampling rate, which does not cause overlapping is the Nyquist Sampling Rate. Given the function is zero for all frequencies higher than $u_b$, $v_b$, we can capture all information with sampling distances w, h along x and y defined as follows:
                
                $$
                w \leq \frac{1}{2u_b} \quad h \leq \frac{1}{2v_b}
                $$
                
        - 2) Quantization – intensity discretization, creates “grey levels”
            
            After the sampling, where a continuous function was discretized and converted to an continuous value, we now need to assign a certain value to each pixel. We therefore create K intervals (normally K = 256), in which we quantizate the signal. This means, that 8 (since $2^8 = 256$) bits are used for each gray pixel. In medical applications, finer quantization might be needed. 
            
        
        ---
        
    - Image Enhancements
        - Noise Surpression
            - SNR
                - Low frequencies have high SNR
                - High frequencies have low SNR
                
                ---
                
                - High frequencies contains noise and edge information
                - Low frequencies contains details
            - Separability
                
                If a 2D kernel is separable, it can be decomposed into a product of two 1D kernels.
                
                $$
                K(i,j) = k_i(i)k_j(j)
                $$
                
                 This separability property can be advantageous in terms of computational efficiency when performing convolution operations. We now have to compute two 1D kernels, instead of one 2D kernel:
                
                $$
                o(i, j) = f(i,j)*I(i,j) = f_1(i)*(f_2(j)*I(i,j))
                $$
                
            - Convolutional linear filters – low-pass convolutional filters
                - Averaging Filter
                    
                    The filter is defined as: 
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2023.png)
                    
                    → Takes away noise, but blurres image
                    
                    → It is separable: 
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2024.png)
                    
                - Binomial Filter
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2025.png)
                    
                    →  Takes away noise, no ripples, but still blurred
                    
                    → It is separable.
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2026.png)
                    
                    If n → $\infty$, we approach the Gaussian filter
                    
                - Gaussian Filter
                    
                    The Gaussian filter is defined as:
                    
                    $$
                    f(x,y) = \frac{1}{2\pi\sigma^2}exp \left(-\frac{1}{2}\left(\frac{x^2+y^2}{\sigma^2}\right) \right)
                    $$
                    
                    , with variance $\sigma$ (the higher $\sigma$, the more blurred the image gets)
                    
                    → set the size of the kernel $K = 2\pi\sigma$
                    
                    → The kernel is separable
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2027.png)
                    
                
                Linear Filters cannot solve the blurring problem
                
            - Non-linear filters - edge-preserving filters
                - Median
                    1. Rank-order neighborhood intensities in a patch of the image
                    2. Take middle value and assign it to the patch center
                    3. Go over all the image in a sliding window
                    
                    → No new grey levels will emerge
                    
                    → Great for robustness to outliers and salt-and-pepper type noise
                    
                    → Great for preserving sharp transitions, high frequency components and, essentially, edges and corners
                    
                    → Median Filtering is better for the following types of noise:
                    
                    - Gaussian noise, i.e., noise distributed by independent normal
                    distribution
                    - Salt and pepper noise
                    - Uniform noise, i.e., distributed by uniform distribution
                    - Exponential noise model
                    - Rayleigh noise
                - Non-local means
                    
                    This algorithm calculates the intensity of each pixel, by comparing the average intensity of its patch with each other path in the image and therefore assigning a weight $w(x, y)$, with a high value for similar patches. 
                    
                    $$
                    I(x) = \sum_{y\in\Omega} w(x,y)J(y) \\ w(x,y) = \frac{1}{Z(x)} \exp \left(-\frac{||J(N(x))-J(N(y))||_2^2}{h^2}\right)
                    $$
                    
                    , with $J(y)$ being the intensity values of the original image at pixel y, $J(N(x))$ being the average intensity of a patch of pixel around x, x and y being pixel locations,  $\Omega$ being the hole image and $Z(x)$ being a normalisation variable, so that the weight is between 0 and 1. 
                    
                    h is a desgin parameter for the amount of smoothing:
                    
                    • Too low h does not remove noise
                    • Too high h leads to over-smoothing
                    
                    - Higher noise suppression
                    - Better preservation of edges
                    - May create artifacts
        - Image Deblurring
            - Unsharp masking filter
                
                Unsharp masking enhances the local contrast and can effectively undo gaussian blur. 
                
                $$
                F_{sharp} = F_{blurred} -(c\Delta t)\nabla^2 F_{blurred}
                $$
                
                In practise $c\Delta t$ is chosen as a constant. 
                
                The algorithm uses the difference between the original image and the smoothed image to sharpen the edges. 
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2028.png)
                
            - Inverse Filtering
                
                We can recover the sharp image with the following formula
                
                $$
                \hat F_{sharp}(u,v) = \frac{F_{blurred}(u,v)}{H(u,v)} + N(u,v)
                $$
                
                $H(u,v)$ is the convolution lowpass filter, that caused the blurring. It can be estimated from a IMU. 
                
                → for high frequencies $(u, v)$:
                
                → $H(u, v) = 0$, $\hat F_{sharp}$ is not recoverable
                
                → Noise $N(u,v)$ in $F_{blurred}(u, v)$ is high
                
                therefore noise (salt and pepper noise) is amplified
                
            - Wiener Filter
                
                With the Wiener Filter we overcome the aforementioned problem. We set the weight for signals at high frequencies (u, v) lower. 
                
                The filter is defined as: 
                
                $$
                W(u, v) = \frac{H(u,v)}{H(u, v)^2+\frac{1}{SNR}}
                $$
                
                $$
                SNR = \frac{|F(u,v)|^2}{|N(u,v)|^2}
                $$
                
                As we neither know $F(u,v)$ nor $N(u,v)$, we can estimate the SNR as a constant.
                
                We calculate the sharpened image as follows: 
                
                $$
                
                \hat F_{sharp}(u,v) = W(u, v) *F_{blurred}(u,v)
                
                $$
                
                With this filter we get the following properties:
                
                $$
                SNR \rightarrow \infty \Rightarrow 1/SNR \rightarrow 0 \Rightarrow F_{sharp} \sim 1/H \\
                SNR \rightarrow 0 \Rightarrow 1/SNR \rightarrow \infty \Rightarrow F_{sharp} \sim 0
                $$
                
                → optimal deblurring technique
                
        - Contrast Enhancement
            
            Usecases:
            
            1) Compensating under-, over-exposure
            
            2) Spending intensity range on interesting part of the image
            
            - Histogram equalization
                
                A histogram captures global brightness information compact (without spatial relation). As we want to enhance the contrast we try to create a flat histogram:
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2029.png)
                
        
        Not relevant for the exam:
        
        - Anisotropic diffusion
        
        ---
        
- Lecture 3: Features
    
    > description of a pattern / object in the image, e.g., shape, texture, emitted heat if infrared
    > 
    
    Challenges of feature detection are: Viewpoint, Illumination, Background, Occlusion, Intra-class variation
    
    - Global Features
        - Appearance-based methods
            - Use the object’s image as the feature vector
            - A simple model of an object is its images, which are called templates
            - Identifying the object in a new image is done via template matching: Shift the template over the image and compare (with sum-of-squared distance [SDD] or normalized cross-correlation [NCC])
            - **Problems:**
                - Variations in image appearance will change the appearance of the template
                - If there are many different objects, you may need to have many templates
                - For recognition, you may need to use many templates on the image to identify the objects
        - PCA (Principal component analysis)
            
            **Idea:** 
            
            In a given set of multi-dimensional data samples, there exists strong correlation across different dimensions. Reduce the dimensionality while retaining as much variation as possible, by removing dimensions with little to now information. 
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2030.png)
            
            **Method:**
            
            1. De-mean the samples
                
                $$
                x_i' = \frac{x_i-\bar x}{\sigma}
                $$
                
            2. Compute the empirical covariance matrix C of the samples
            
            $$
            C = \frac{1}{n-1} \sum_{n=1}^N(x_nx_n^T)
            $$
            
            1. Apply eigenvalue decomposition on the matrix C and resulting
            eigenvectors are the principal directions
                
                $$
                Cv_i = \lambda_iv_i
                $$
                
            2. Order the components based on the magnitude of the eigenvalues
            3. Form a projection matrix P using only the first *k* eigenvectors as columns and project it onto the original image
                
                Projected Data $I_{compr} = I_{original}P$
                
                Most of the time, $k = 10$ is enough, to still keep 85 % of the information.
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2031.png)
                
            
            Special Cases: N = number of pixels, d dimensions
            
            - $N > d$: C will be full rank positive semi-definite matrix and admit 𝑑 real non-negative eigenvalues and corresponding eigenvectors, which are called the principal directions
    - Local Features
        - 1) Identifying points of interest
            - Edge Detection
                
                Edges have large changes in one direction
                
                - Gradient operators
                    
                    Locate $f(x,y)$ steepest slopes using gradient magnitude
                    
                    $$
                    \underbrace{\theta = arctan \left(\frac{\partial f/\partial y}{\partial f/\partial x}\right)}_{angle \space of \space maximum \space change} \quad 
                    
                    \underbrace{|f'(x,y)|= \sqrt{\left(\frac{\partial f}{\partial x}\right)^2+{\left(\frac{\partial f}{\partial y}\right)^2}}}_{magnitude \space of \space maximum \space change}
                    $$
                    
                    Since a function is continuous, but the intensities of an image are discrete, one can approximate the derivatives with:
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2032.png)
                    
                    A more robust (to noise) derivative approximation are Sobel masks:
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2033.png)
                    
                    **Drawbacks:** 
                    
                    1) Gaps
                    
                    2) Several pixel thick at places 
                    
                    3) Some edges are weak whereas others are salient (hervorstechend)
                    
                - Zero crossings of Laplacian
                    
                    Instead of using the first derivative (as in the chapter “gradient operators” one could use the second derivative to find edges. edges can be found, if the second derivative is = 0. The second derivative is defined as: 
                    
                    $$
                    \nabla ^2f = \frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2} = 0
                    $$
                    
                    In discrete domain, the second derivative can be approximated by: 
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2034.png)
                    
                    The left approximation neglect edges with an angle at  $\pm$$45^{\circ}$, $135^{\circ}$ and $225^{\circ}$, therefore the right approximation is more isotropic. 
                    
                    As this operator is highly sensitive to noise, the image should be smoothed before. Using a Gaussian Filter $G$ to smooth the image before and combining it with the Laplacian  $L$ operator, we get a Laplacian of Gaussian (LoG) filter, which has the form of a inversed Mexican hat. 
                    
                    $$
                    LoG = L*(G*f) = (L*G)*f
                    $$
                    
                    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2035.png)
                    
                    Gradient operator vs Laplacian zero crossing: 
                    
                    | Gradient | Laplacian |
                    | --- | --- |
                    | provides location, magnitude and direction of the edge | provides location of the edge |
                    | detection using Maxima Threshold | detection using zero crossing |
                    | non linear operation | linear operation |
                    | requires two convolution  | requires one convolution |
                    
                    Further advantages of this method are:
                    
                    - one-pixel thick edges
                    - closed contours
                - Canny Edge Detection
                    
                    1) Smoothing of the image by using a Gaussian filter (→ Lecture 2 → Image enhancement)
                    
                    2) Compute the gradient magnitude and gradient direction at each pixel
                    
                    3) Apply a non-maximum suppression to thin edges (→ Post-processing)
                    
                    4) Apply Hysteresis thresholding (→ Post-processing)
                    
                    **Advantages:**
                    
                    - State of the art edge detector
                    - very efficient implemnetation
                    
                    **Disadvantages:** 
                    
                    - Fails with corners and crossings
                    - Gaps can still be created
                - Post-processing
                    
                    **Non-maximum suppression:** 
                    
                    For each pixel in the edge map, the gradient direction is calculated and quantisized to certain angles. Each pixel’s gradient magnitude is then compared to its neighbours in gradient direction. If the gradient magnitude of the current pixel is greater then the strengths of its neighbours, it is considered as a local maxima, otherwise it is suppressed (set to zero).
                    
                    → gives one thick line for each edge
                    
                    **Hysteresis thresholding:** 
                    
                    Two threshold values $t_{low}$ and $t_{high}$  are defined. Each pixel is then put into one of the following classes: 
                    
                    - Class 1: $|f’(i, j)| \geq t_{high}$
                    - Class 2: $|f’(i, j)| < t_{low}$
                    - Class 3: $t_{high} > |f’(i, j)| \geq t_{low}$
                    
                    Class 2 pixels are discarded. Class 3 pixels are kept only, if they are connected to a class 1 pixel(also through other class 3 pixels)
                    
                
                Not relevant for the exam:
                
                - SIFT
                - SURF
                - FAST
            - Corner Detection
                
                Corners have large changes in two directions
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2036.png)
                
                - Harris Corner Detection
                    
                    **Algorithm:** 
                    
                    1) Compute the gradient of the image at each pixel in each direction using gradient operators (Sobel mask etc.) resulting in $I_x$  and $I_y$
                    
                    2) Calculate the structure tensor M:
                    
                    $$
                    M =\sum_{neighbourhood}
                    \begin{bmatrix}I_x^2 & I_xI_y\\I_xI_y&I_y^2\end{bmatrix}
                    $$
                    
                    The structure tensor M includes information of the gradients in the neighbourhood. 
                    
                    3) Calculate the eigenvectors and eigenvalues of M. The eigenvectors define the direction and the eigenvalues the size of observed changes in intensity:
                    
                    - Regions with homogeneous intensity have two small eigenvalues
                    - Regions with an edge have one large and one small eigenvalue
                    - Regions with a corner have to large eigenvalues
                        
                        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2037.png)
                        
                    
                    4) For corner detection, the response function R is defined:
                    
                    $R = det(M)-k(trace(M))^2 = \lambda_1\lambda_2-k(\lambda_1+\lambda_2)^2$
                    
                    A high value of $R$ indicates a corner. $k$ is a constant.
                    
                    5) Non maximum suppression of the neighbourhood 
                    
                    6) Threshold
                    
            - Blob detection
                
                **Definition:** A blob is an area of an image where some properties are roughly
                costant and differ from their surrounding
                
                **Detection: Laplacian of Gaussian (LoG):** 
                
        - 2) Extract local features around the interest points
            
            A vector of patches is called a descriptor. In the matching step, the descriptor can then be compared. 
            
            **Descriptor:**
            
            **Creation of a descriptor:** 
            
            1) Define an area – local patch – in the image around each interest point
            
            2) Extract features from the local patch to represent each interest point with a vector of numbers
            
            **Requirements:** 
            
            - Invariance under geometric / photometric changes
- Lecture 4: Light
    - Photometry: subjective impressions
    - Radiometry: objective, physical measurements
    - Color Spaces
        - HSV Color Space
            
            1) Hue: Overall colortype
            
            2) Saturation: Vividness of the colour 
            
            3) Value/Brightness
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2038.png)
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2039.png)
            
        
        ---
        
    - Representation of color
        
        The human cones can detect 3 colors with relative **sensitivity $H_{1}, H_{2}, H_{3}$**
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2040.png)
        
        A source with **spectral radiant lux $C(λ)$** results in a **response $R$**:
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2041.png)
        
        This response R must be recreated by a display with **primaries $P_{j}$** (RGB, plasmas of screen) and their corresponding **amount m**. 
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2042.png)
        
        Those two systems of equations can be solved, if the primaries P and H are linearly independent. 
        
        ---
        
    - Tristimulus values
        
        As a reference to the amount m, “white” w is choosen. The relative values T of m with respect to w can be calculated as follows:
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2043.png)
        
        In the picture below, one can read out the values Ti for a desired monochromatic source $C(λ)$. Negative values are physically not possible. 
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2044.png)
        
        ---
        
    - Chromaticity Values and Coordinates $(r, g)$
        
        By normalizing the Tristimulus values, one can eliminate the brightness information. 
        
        $$
        t_j = \frac{T_j}{T_1+T_2+T_3}
        $$
        
        since $t_1 + t_2 + t_3 = 1$ → linear dependency and therefore only 2 DoF’s:
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2045.png)
        
        Resulting in the r-g-colorspace, where only the grey triangle can be reached (no negative values):
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2046.png)
        
        ---
        
    - Texture
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2047.png)
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2048.png)
        
        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2049.png)
        
        - Fourier Features
            
            Collects information over the image globally
            
            Fourier power spectrum: $\int_A \int|F(u, v)|^2dudv$
            
            - Regularity → dominant peaks
            - Orientation → power spectrum integrated over sectors provides orientation information
            - Coarseness → power spectrum integrated over rings around origin provides coarseness information
        
        ---
        
    - Eigenfilters
- Lecture 5: Image segmentation, classification and object recognition
    - Image segmentation
        
        > Identify groups of pixels that belong together
        > 
        
        Segmentation as grouping:
        
        - k-means
            
            1) Randomly initialize the cluster centers $c_1, c_2, …c_k$
            
            2) For each point p, find the closest $c_i$, put p into cluster i
            
            3) Given points in each cluster, solve for $c_i$
            
            4) If $c_i$ have changed, repeat Step 2
            
            → Does not always find a global minimum
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2050.png)
            
            **Feature Space:**
            
            Depending on what we choose as the feature space, we can group pixels in different ways (position, RGB values, texture etc)
            
            Pros:
            
            - Simple, fast to comput
            - Converges to local minimum of within-cluster squared error
            
            Cons:
            
            - Setting k
            - Sensitive to initial centers and outliers
            - Detects spherical cluster only
        - Mixture of Gaussians, EM
            
            This model assumes that each pointcloud/blob can be described as a normal distribution, with means $μ_b$, covariance $V_b$ and dimension $d$
            
            $$
            P(x|\mu_b, V_b) = \frac{1}{\sqrt {(2\pi)^d|V_b|}}e^{-\frac{1}{2}(x-\mu_b)^TV_b^{-1}(x-\mu_b)}
            $$
            
            We now want to maximize the likelihood of observing x in a blob, by finding the blob parameters $μ_b$ and $V_b$:
            
            $$
            P(x|\theta) = \sum_{b=1}^K \alpha_b P(x|\theta_b), \theta = [\mu_1,...,\mu_K, V_1,...,V_K]
            $$
            
            This is done using the following algorithm:
            
            1) E-Step: Given current guess of blobs, compute probabilistic ownership of each point
            
            $$
            P(b|x, \mu_b, V_b) = \frac{\alpha_bP(x|\mu_b, V_b)}{\sum_{i=1}^K\alpha_iP(x|\mu_i, V_i)}
            $$
            
            2) Given ownership probabilities, update blobs to maximise likelihood function
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2051.png)
            
            Pros:
            
            - Probabilistic interpretation
            - Soft assignments between data points and clusters
            - Generative model, can predict novel data points
            - Relatively compact storage $O(kd^2)$
            
            Cons:
            
            - Local minima
            - Need to know number of components K
        
        Interactive segmentation with GraphCuts
        
        - Markov Random fields
            
            Markov Random Fields (MRFs) are a type of probabilistic graphical model used for modeling complex systems with random variables that exhibit dependencies. To display the graphical model, one uses nodes (random variables) and edges (dependencies between variables). Each variable can take on a set of possible values. Nodes connected by an edge are called neighbors.
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2052.png)
            
            - observed pixel intensities (generated by a probabilistic model)
            - hidden variable, which is dependent on its neighbours (i.e. foreground, background)
            - dependence of hidden variables on neighbours $\Psi(x_i, x_j)$, the state-state compatibility function
            - Relation between hidden variabel and observed pixel intensity $\phi(x_i, y_i)$, the image-state compatibility function
            
            The field joint probability distribution is defined as:
            
            $$
            P(x,y) = \prod_i\Phi(x_i, y_i)\prod_{i,j}\Psi(x_i, x_j)
            $$
            
            This can be rewritten to a minimisation problem by taking the negative logarithm:
            
            $$
            E(x, y) = \sum_i\varphi(x_i,y_i)+ \sum_{i,j}\psi(x_i, x_j)
            $$
            
            This function is called an energy function. 
            
            **GraphCuts:** 
            
            1) We convert MRF into a source-sink graph: We take some labelled pixels and say, that one is a sink $D_p(s)$, the other is a source $D_p(t)$ (i.e. foreground/background) → hard constraint
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2053.png)
            
            2) We now want to minimise the following energy function:
            
            $$
            E = \sum_pD_p(L_p)+\sum_{pq \in N}w_{pq} \delta(L_p \neq L_q)\\
            w_{pq} = exp \left( -\frac{\Delta I_{pq}}{2\sigma^2} \right) \\
            L_p = L_q \rightarrow \delta = 0 \\ L_p \neq L_q \rightarrow \delta = 1
            
            $$
            
             $D_p$  is assigning each pixel to the foreground or background, $p$ and $q$ are neighbouring pixels, $\Delta I_{pq}$ is the distance between two pixels, $w_{wq}$ is the weight which is high for pixels nearby, $L_p \in (s, t)$ and $\sigma$  can be choosen. 
            
        
        ---
        
    - Classification
        
        > A procedure that takes the features as input and predicts the segmentation label / class assignment
        > 
        - K nearest neighbours - KNN
            
            This algorithm takes the test point and looks at the K-nearest neighbours of its training dataset (for which we know the labels). The label of the test point is then defined by the most common label of the neighbours.  
            
            **Advantages:**
            
            - simple to implement
            - simple to understand
            - distance definition is flexible
            
            **Disadvantages:** 
            
            - Highly dependent on K
            - A lot of training data is required for high dimensional problems
        
        ---
        
    
    A specific object is an instance of a class for example Marc in the class “people”. 
    
    - Specific object recognition
        - Model-based
            
            We have a model 3D of the object. We determine the specific features such as edges and corners in both the image and the object. We estimate the pose of the object in the scene. We then match the extracted features.
            
        - Image-based
            
            Shift the template (2D) over the image and compare → Problem of variation in the appearance (viewpoint/lighting)
            
        - Hybrid models
            
            **Euclidean invariant feature:**
            
            Training: 
            
            1) Look for corners (Harris detector)
            
            2) Take circular regions around these points,with different radii
            
            3) Calculate the features from those regions, which are invariant under planar rotation
            
            4) Do step 1to 3 for different viewpoints
            
            Testing:
            
            1) Compare the found features with those found for images in a database and find matches
            
            2) Look for consistent placement of candidate matches / geometric constraints (epipolar geometry)
            
            3) Decide which object based on the number of remaining matches
            
        - Visual words
            
            We are given a set of features of an image in which we want to recognise objects, and a codebook of visual words. The codebook was created using a lot of data and k-means. Each cluster of points in the k-space was assigned to a visual word. The new set of features is now put into k-means and the closest visual word is assigned to each local feature. 
            
        - RANSAC
            
            There are two types of RANSAC algorithms; 
            
            - test on epipolar geometry
                - Assumes that there is a fundamental matrix
                - Assumes rigidity of scene → objects do not move with respect to each other
            - test on projectivities
                - Assumes that there is a projectivity that maps points in the first image onto the the matching point in the second image
                - Asssumes rigidity
                - Assumes that the scene is largely planar
            
            **Algorithm:**
            
            1) Randomly choose $s$ samples, where $s$ is the minimum samples to fit the model (for a line $s$ = 2 samples, plane  $s = 3$)
            
            2) Fit the model $H$ to the randomly chosen samples (for a line, fit a line through the two point)
            
            3) Count the number $M$ of data points (inliers), that fit the model within a measure error $\epsilon$ 
            
            4) Repeat step 1-3 $N$ times
            
            5) Choose the model that has the largest number $M$ of inliers
            
            > Good to now:
            - robust
            - non deterministic
            > 
        
        ---
        
    - Object category recognition
        - Bag of visual words (BoW)
            
            Similar to visual words, but we now collect all visual words and summarise the entire image based on is histogram of word occurrences. 
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2054.png)
            
            → Works pretty well for whole-image classification
            
        - Sliding windows (so unnötig)
            
            We slide a binary classifier through the hole image and decide at each location, if the object we are looking for is at this place.
            
- Lecture 6: 3D Acquisition
    
    ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2055.png)
    
    - Passive
        - **Multidirectional**
            
            Stereovision:
            
            - Camera
                
                A camera projects a the world from 3D to a 2D plain. This transformation is described with an intrinsic and an extrinsic transformation. 
                
                We use 4 coordinate systems; The world coordinate system, the camera coordinate system, the image coordinate frame and the pixel coordinate system. 
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2056.png)
                
                Extrinsic matrix: from world coordinate frame to camera coordinate frame with the rotation matrix $R$ and the position  $C$ of the camera in world frame:
                
                $$
                \begin{pmatrix}X_C\\Y_C\\Z_C\end{pmatrix} = R_{3\times3} \begin{pmatrix}X_W-C_x\\Y_W-C_y\\Z_W-C_z\end{pmatrix}
                $$
                
                Intrinsic matrix: from camera coordinate frame to image coordinate frame, with focal length $f$:
                
                $$
                \begin{pmatrix}x_i\\y_i\\Z_C\end{pmatrix} = \begin{pmatrix}f&0&0\\0&f&0\\0&0&1\end{pmatrix}\begin{pmatrix}X_C\\Y_C\\Z_C\end{pmatrix} , i = l, r
                $$
                
                Intrinsic matrix: from image coordinate frame to pixel coordinate frame, with $k_x$  and $k_y$ the pixel per length in x and y direction of the image sensor, $x_0$, $y_0$ the offset between the image plane origin and the outer top pixel and $s(\theta)$ the skew of the image plane (which we ignore from now on) , and $\tau$ a parameter which is a scale parameter which can be choosen.
                
                $$
                \tau\begin{pmatrix}u\\v\\1\end{pmatrix} = \begin{pmatrix}k_x&s&x_0\\0&k_y&y_0\\0&0&1\end{pmatrix}\begin{pmatrix}x_i\\x_i\\Z_C \end{pmatrix}
                $$
                
                Combined, we can formulate a transformation from a 3D point in world coordinates to the pixel coordinate frame:
                
                $$
                \tau\begin{pmatrix}u\\v\\1\end{pmatrix} = \begin{pmatrix}k_xf&0&x_0\\0&k_yf&y_0\\0&0&1\end{pmatrix} R_{3\times3}\begin{pmatrix}X_W-C_x\\Y_W-C_y\\Z_W-C_z\end{pmatrix} \\
                \tau p = KR^T(P-C)
                $$
                
                Source: [https://towardsdatascience.com/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec](https://towardsdatascience.com/what-are-intrinsic-and-extrinsic-camera-parameters-in-computer-vision-7071b72fb8ec)
                
            - Simple configuration
                
                ---
                
                Assumptions:
                
                - identical cameras
                - coplanar image planes
                - aligned x-axes
                - coordinate system of the left camera aligns with the world coordinate system
                
                ---
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2057.png)
                
                The coordinates of a point P(X, Y, Z) are then given as:
                
                $$
                X = b\frac{x_l}{(x_l-x_r)} \\ 
                Y = b\frac{k_x}{k_y}\frac{y}{(x_l-x_r)}\\
                Z = bk_x\frac{f}{(x_l-x_r)}
                $$
                
                with disparity $(x_l-x_r)$. Note that for this case $y = y_l = y_r$ and that $x_i$ and y are centered in the center of the image plane. 
                
                The depthresultion can be increased by:
                
                - increase b
                - increase ƒ
                    
                    → But this results also in a reduction of visibility
                    
            - General Setup
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2058.png)
                
                Rewriting the equations we derived in the chapter before, we get:
                
                $$
                P = C + \tau RK^{-1}p
                $$
                
                By adding a second camera, we get:
                
                $$
                P = C' + \tau' R'K'^{-1}p'
                $$
                
                We now have 6 equations an 5 unknowns ($X, Y, Z, τ, τ$’): 
                
                Combining these two equations we get the ray equation, which describes the epipolar line with free parameter  $\tau$ ($\tau’$ depends on $P$ and therefore depends on $\tau$)
                
                $$
                \tau' p' = \tau K'R'^TRK^{-1}p+K'R^T(C-C')
                $$
                
                The first part of this equation $K'R'^TRK^{-1}p$ is the projection of the ray’s point at infinity, called the vanishing point. The second part of the equation $e’ \tau_e'= K'R^T(C-C')$ is the projection $e’$  of the first camera’s center on the second camera’s image plane, called the epipole. 
                
                Taking the first part of the equation, we can define the infinity homography matrix A:
                
                $$
                A = \frac{1}{\tau_e'}K'R'^TRK^{-1}
                $$
                
                This can be used to simplify the ray’s equation to describe the epipolar line:
                
                $$
                \tau' p' = \tau_e'(\tau Ap+e')
                $$
                
                The epipolar line is the line in the image plane of the camera, on which the Point P must lie, given it’s observation in the other camera. 
                
                Simplification (ka was die da möchet) leads to: 
                
                $$
                p'^T Fp = 0, F = \begin{pmatrix} 0 &-e_3'&e_2'\\e_3'&0&-e_1'\\-e_2'&e_1'&0\end{pmatrix}A
                $$
                
                , with fundamental matrix F
                
                > Nice to know:
                F can be computed without knowledge of the cameras by 8 corresponding points
                F is often found using RANSAC
                > 
            - Correspondences search
                
                In the second image the point must be along the projection of the viewing ray for the first camera, which is described by the epipolar constraint:

                $$
                \tau' p' = \tau K'R'^TRK^{-1}p+K'R^T(C-C')
                $$
                
                - Correlation based search
                    
                    Take a patch of pixel around the point in the first image. The patch is shifted over the second image to search for the highest correlation. At this point, the corresponding point is assumed (small patch, fast but noisy | large patch, slow and bad comput. but fast)
                    
                - Feature Based
                    
                    Seraching for corners/edges in both images and try to find their correspondences (usually little such features exits, might have multiple features on the epipolar line)
                    
                
        - **Unidirectional**
            - Silhouettes
                - Tracking of turntable rotation
                - Volumetric modelling from silhouettes
                - Triangular textured surface mesh
            - Shading
                - Directional lighting
        
        ---
        
    - Active
        - **Multidirectional**
            - Line Scanning
                
                One of the two cameras is replaced by a laser. As two lines in 3D usually do not intersect, a lens is placed in front of the laser, so that the laser produces a plane instead of a point.
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2060.png)
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2061.png)
                
            - Structured Light
                
                A patterns of a special shape are projected onto the scene. The deformations of the patterns yield information on the shape.
                
                - Pattern
                    - Serial binary pattern:
                        
                        $n$  patterns → $2^n$ patterns 
                        
                        ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2062.png)
                        
                    - Serial color pattern:
                        
                        Instead of black and white pattern, colored patterns are used
                        
                        $n$ patterns → $3^n$ patterns 
                        
                    
            - Photometric stereo
                
                By using different sources of light from different positions and angles the objects surface normals are estimated. By using different colored light sources, the lighting can happen at the same time. 
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2063.png)
                
        - **Unidirectional**
            - Time-of-flight
                
                By measuring the time, it takes pulsed electromagnetic waves to travel from the sensor to the object and back, the objects distance can be calculated. 
                
                Used waves:
                
                - radar
                - sonar
                - optical radar
                
        
        ---
        
- Lecture 7: Motion Tracking
    
    > Good to know:
    $\nabla f= \begin{bmatrix} f_x\\ f_y \end{bmatrix}$
    > 
    - Motion Estimation
        
        > Determine the general motion across a sequence of images, which can be trough motion of something/someone in the scene or the motion of the camera. The resulting motion vector can then be used for tracking.
        > 
        - Optical Flow
            
            **Assumptions:** 
            
            1) intensity information of a pixel should not change along the motion
            
            2) Displacements and time-steps are small
            
            Therefore, the optical flow is the apparent motion of brightness patterns 
            
            Since the intensity of a pixel is not changing, the following equation holds for each pixel:
            
            $$
            \frac{dI}{dt} = \frac{\partial I}{\partial t}\frac{dx}{dt}\frac{\partial I}{\partial x} + \frac{dy}{dt}\frac{\partial I}{\partial y} + \frac{\partial I}{\partial t} = uI_x + vI_y+I_t = [u, v]^T \nabla I + I_t = 0
            $$
            
            with $I(x,y,t)$ brightness at (x, y) at time t, $dI/dt$ the change of intensity when following a physical point through the images, $∂I/∂t$ the change of intensity when looking at the same pixel (x, y) through images ignoring its movement, $dx/dt$ the horizontal change of the pixel, $dy/dt$ the vertical change of the pixel 
            
            → Since the equation is dependent on the gradient u, v, this can lead to misleading  results: 
            
            - untextured, rotation sphere $u, v = 0$
            - no motion but changing lighting $u, v ≠ 0$
            
            → $I_x$, $I_y$ and $I_t$ can be measured, but $u, v$  are unknown → 1 equation, 2 unknowns
            
            **Aperture Problem**: 
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2064.png)
            
        - Horn & Schunck Algorithm
            
            To overcome this ill defined problem this algorithm enforces smoothness of the displacement field:
            
            $$
            L_s = \int \int ((u_x^2+u_y^2)+(v_x^2+v_y^2))dxdy
            $$
            
            with $u_x = \frac{du}{dx}, u_y, v_x, v_y$ being the spatial derivatives of the displacement vector at a point
            
            Small $L_s$ means smoother field, which leads to the minimisation problem:
            
            $$
            arg_{u,v} min(L) \\ L = L_c + \lambda L_s \\= \int \int (uI_x + vI_y+I_t)^2dxdy + \lambda \int \int ((u_x^2+u_y^2)+(v_x^2+v_y^2))dxdy
            $$
            
            with $\lambda$ being parameter between constraint and smoothness.
            
            > **Good to know**
            > 
            > 
            >  $\lambda$ should be large, if brightness measurements are accurate, and small, if they are noisy
            > 
            
            This minimisation problem can be solved iteratively by using Euler Lagrange equations, which leads to the equations below. 
            
            **Algorithm:**
            
            1) Compute image gradient and temporal derivatives: $∇𝐼$ and $𝐼_t$ 
            
            2) Initialize $u^0 = 0, v^0 = 0$
            
            3) At each iteration 𝑛 update the displacement vector field at each pixel by
            
            $$
            u^{n+1} = u^n-\tau (I_x([u, v]^T\nabla I + I_t)-2\lambda \nabla^2u \\ v^{n+1} = v^n-\tau (I_x([u, v]^T\nabla I + I_t)-2\lambda \nabla^2v
            $$
            
            4) Stop when the field is not changing anymore
            
    - Tracking
        
        > Follow the movements of something/somebody → no aim for a dense displacement field
        > 
        - Track a point
            
            We want to minimise this energy with respect to $u, v$:
            
            $E(u, v) = (I_0(x-u, y-v)-I_1(x,y))^2$
            
            where $I_0$ is the Intensity at time  $t$ and $I_1$ is the Intensity at time $t+dt$
            
            If we have to functions $f(x)$ and $g(x)= f(x-h)$  and we want to track point x on $f(x)$, the displacement u is given by: 
            
            $$
            u = \frac{f(x)-g(x)}{f'(x)}
            $$
            
            The displacement is only defined, if $f'(x) \neq 0$. By assuming motion in a region to be equal, we get a new formula, which advantage is, that it is less likely to have a denominator equal to zero. 
            
            $$
            u^* = \frac{\sum_x f'(x)(f(x)-g(x))}{\sum_x (f'(x))^2}
            $$
            
            We may not find the shift u in the first shot, so we iterate: 
            
            **Algorithm:** 
            
            1) We start by setting $u^0 = 0$
            
            2) Update u: $u^{n+1} = u^n+ \frac{f(x-u^n)-g(x)}{f'(x-u^n)}$
            
            3) Repeat until convergence 
            
            → Problems with local minimas: It can not be said in which direction the function moved
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2065.png)
            
            The problem can be solved by choosing the framerate higher then the “half-wavelength” (Nyquist rate)
            
        - Track a point in 2D
            
            To track a point in 2D, we have the same equation as we derived in “Optical Flow” just for one point. We therefore still have the aperture problem! To overcome this problem we now assume single motion in a region. This results in the following equation for the displacement u, v of a single point:
            
            $$
            \begin{bmatrix} u^* \\ v^* \end{bmatrix} = H^{-1} \sum_{x,y}\nabla I_0(I_0(x,y)-I_1(x,y)),\quad  \\ H = \sum_{x,y} \nabla I_0 \nabla I_0^T = \begin{bmatrix}
                \sum_{\text{local window}} \left(\frac{\partial I}{\partial x}\right)^2 & \sum_{\text{local window}} \frac{\partial I}{\partial x} \frac{\partial I}{\partial y} \\
                \sum_{\text{local window}} \frac{\partial I}{\partial x} \frac{\partial I}{\partial y} & \sum_{\text{local window}} \left(\frac{\partial I}{\partial y}\right)^2
            \end{bmatrix}
            $$
            
            → This can only be solved, if H is invertible 
            
            H is equal to the structure tensor, as in the Harris corner detection. It is advantageous for tracking to have large eigenvalues.
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2066.png)
            
            > **Good to know:**
            Flat regions and pure edges lead to non invertible structure tensor as both eigenvalues will be 0.
            Both eigenvalues need to be non-zero for H to be invertible, which is satified by corners.
            > 
            
            We may not find the exact shift u and v in the first shot, so we iterate:
            
            **Algorithm:** 
            
            1) Set $u^0= v^0 = 0$
            
            2) Update u, v:
            
            $$
            \begin{bmatrix} u^{n+1} \\ v^{n+1} \end{bmatrix} = \begin{bmatrix} u^{n} \\ v^{n} \end{bmatrix} + H^{-1} \sum_{x,y}\nabla I_0(I_0(x-u^n,y-v^n)-I_1(x,y))
            $$
            
            3) Repeat until convergence 
            
        - Track a bigger box - Template Tracker
            
            Until now, we assumed pure translation. This cannot be assumed since objects also can rotate and scale. 
            
            For this algorithm, the motion is parameterised by p. 
            
            $$
            E(u, v) =\sum_x (I_0(x-u, y-v)-I_1(x,y))^2 \\= \sum_x (I_0(W(x, y; p))-I_1(x,y))^2 = E(p)
            $$
            
            with transformation $W(x,y;p)$
            
            **Lucas-Kanade Template Tracker:** 
            
            1) Initialize with $p^0 = 0$
            
            2) Transform template to get $I_0(W(x;p^n))$
            
            3) Update  $p^{n+1} = p^n + \Delta p^*$
            
            $$
            \Delta p^* = H^{-1} \sum_x \frac{\partial W(x;p)^T}{\partial p} \nabla I_0(I_1(x)-I_0(W(x;p))\\
            H = \sum_x \frac{\partial W(x;p)^T}{\partial p} \nabla I_0 \nabla I_0^T \frac{\partial W(x;p)}{\partial p} 
            $$
            
            4) Repeat until convergance
            
            Pros:
            
            - Can handle different parameter spaces and different transformations
            - Can converge fast in a high-frame rate video
            
            Cons: 
            
            - Not robust to image noise
            - May not work well through occlusions
            - Not robust to large displacements
        - Track by detection/segmentation
            - Track a specific target
                
                **Algorithm:** 
                
                1) Detect keypoints (edges, corners etc.) which are invariant to rotations, scale, and perspective of target
                
                ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2067.png)
                
                2) Build Feature Descriptors (Histograms of oriented gradients)
                
                3) Match Keypoint Descriptors in the other image
                
                4) Outliers are eliminated 
                
                5) 3D pose is calculated (RANSAC)
                
            - Track an object class
                
                > Track all objects of the same category → no specific features that identifies an object
                > 
                
                There are two ways of associating objects:
                
                1) Learn an appearance model to measure the similarity directly
                
                2) Predict the motion of the objects from one image to the next to reduce search space (→ Predict the motion)
                
                To detect classes of objects, supervised learning methods are used, such as:
                
                - k-nearest neighbours
                - CNN’s
                - Decision Trees
        - Model motion
            
            → to predict location and reduce search space
            
            **General framework:**
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2068.png)
            
            $x_t = f_{t-1}(x_{t-1}, w_{t-1}):$ Motion model at time $t-1$ with noise $w_{t-1}$
            
            $z_t = h_t(x_t, v_t):$ Observation model at time $t$ with noise $v_t$
            
            **Recursive Bayesian Filter:** 
            
            Using the Bayes’rule we can formulate the following update rule for the probability of being at $x_t$, given the observations $Z_t$:
            
            $$
            p(x_t|Z_t) = \frac{p(z_t|x_t)}{p(z_t|Z_{t-1})}p(x_t|Z_{t-1})
            $$
            
            with the probability of observing the given observation $z_t$ at the position $x_t$: $p(z_t|x_t)$ 
            
            and $Z_t = (z_t, z_{t-1}…z_1)$
            
            **Kalman Filter:**
            
            The Kalman Filter uses the prior knowledge and a physical model to predict its next position (with uncertainty). It then takes the measurement (also with uncertainty) to update its prediction of its current position.
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2069.png)
            
            →  Online Learning
            
        - Online Learning
            
            Online learning allows the appearance model to adapt to changes in lighting conditions, viewpoint, and other factors that may affect the visual appearance of the tracked object. The model continuously updates its parameters and features, such as color histograms, texture patterns, or other visual descriptors, based on new observations, making it more robust in handling variations over time. Rather than training the appearance model from scratch each time, online learning involves updating the model incrementally as new data becomes available, which makes it computationally more efficient. 
            
            ![Untitled](ZF%20a2b272b258b14b15b361653cbabc9e08/Untitled%2070.png)
            
            A problem that can occur, is gradual drift. Gradual drift is the phenomenon, where the estimated position or appearance model of the tracked object slowly deviates from the true state over time. The only thing we are sure about, is the objects initial model. We can correct our current model with this information. 
        